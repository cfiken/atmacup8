{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport # profile report を作る用\n",
    "from matplotlib_venn import venn2 # venn図を作成する用\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from mykaggle.metric.mse import rmse\n",
    "from mykaggle.util.ml_logger import MLLogger\n",
    "from mykaggle.lib.lgbm_util import compute_importances, save_importances\n",
    "from mykaggle.util.routine import fix_seed\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = yaml.safe_load('''\n",
    "name: '057_te_year_pf'\n",
    "competition: atmacup8\n",
    "seed: 1019\n",
    "training:\n",
    "    validation: 'group' \n",
    "    num_folds: 5\n",
    "    num_rounds: 10000\n",
    "    early_stopping_rounds: 1000\n",
    "    verbose_eval: 20\n",
    "    sample_weight_division: 0.01  # 10/2/1\n",
    "    sample_weight_threshold: 1000\n",
    "lgbm_params:\n",
    "    objective: regression\n",
    "    learning_rate: 0.01\n",
    "    max_depth: -1\n",
    "    num_leaves: 31\n",
    "    colsample_bytree: .7\n",
    "    metric: \"None\"\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(settings['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path('../data/')\n",
    "ckptdir = Path('../ckpt/') / settings['name']\n",
    "if not ckptdir.exists():\n",
    "    ckptdir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(datadir / 'id_train.csv')\n",
    "df_test = pd.read_csv(datadir / 'id_test.csv')\n",
    "df_submission = pd.read_csv(datadir / 'atmaCup8_sample-submission.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target や weight の用意\n",
    "target_columns = ['Global_Sales']\n",
    "train_target_columns = target_columns + ['NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "\n",
    "for c in train_target_columns:\n",
    "    df_train.loc[:, 'mod_' + c] = df_train.loc[:, c].apply(lambda x: np.log1p(x))\n",
    "df_train['target'] = df_train.loc[:, 'mod_Global_Sales']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_train = df_train.loc[:, ['id']]\n",
    "df_base_test = df_test.loc[:, ['id']]\n",
    "\n",
    "train_others = {\n",
    "    'main': df_train.copy(),\n",
    "    'another': df_test.copy()\n",
    "}\n",
    "test_others = {\n",
    "    'main': df_test.copy(),\n",
    "    'another': df_train.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mykaggle.feature.le import LE\n",
    "from mykaggle.feature.score import Score\n",
    "from mykaggle.feature.score_tbd import ScoreTBD\n",
    "from mykaggle.feature.multi_platform import MultiPlatform\n",
    "from mykaggle.feature.other_platforms import OtherPlatforms\n",
    "from mykaggle.feature.ce import CE\n",
    "from mykaggle.feature.original import Original\n",
    "from mykaggle.feature.dev_to_pub import DevToPub\n",
    "from mykaggle.feature.pub_to_dev import PubToDev\n",
    "from mykaggle.feature.pub_to_genre import PubToGenre\n",
    "# from mykaggle.feature.pub_to_year import PubToYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = Original(train=True)\n",
    "original_test = Original(train=False)\n",
    "le_train = LE(train=True)\n",
    "le_test = LE(train=False)\n",
    "score_train = Score(train=True)\n",
    "score_test = Score(train=False)\n",
    "score_tbd_train = ScoreTBD(train=True)\n",
    "score_tbd_test = ScoreTBD(train=False)\n",
    "mp_train = MultiPlatform(train=True)\n",
    "mp_test = MultiPlatform(train=False)\n",
    "op_train = OtherPlatforms(train=True)\n",
    "op_test = OtherPlatforms(train=False)\n",
    "ce_train = CE(train=True)\n",
    "ce_test = CE(train=False)\n",
    "dtp_train = DevToPub(train=True)\n",
    "dtp_test = DevToPub(train=False)\n",
    "ptd_train = PubToDev(train=True)\n",
    "ptd_test = PubToDev(train=False)\n",
    "ptg_train = PubToGenre(train=True)\n",
    "ptg_test = PubToGenre(train=False)\n",
    "# pty_train = PubToYear(train=True)\n",
    "# pty_test = PubToYear(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f_train = df_base_train.copy()\n",
    "df_f_test = df_base_test.copy()\n",
    "\n",
    "df_f_train = original_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = original_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = le_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = le_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = score_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = score_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = score_tbd_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = score_tbd_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = mp_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = mp_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = op_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = op_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ce_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ce_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = dtp_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = dtp_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ptd_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ptd_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ptg_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ptg_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "# df_f_train = pty_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "# df_f_test = pty_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "\n",
    "df_f_train.shape, df_f_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some columns\n",
    "df_f_train = df_f_train.drop(['le_Developer'], axis=1)\n",
    "df_f_test = df_f_test.drop(['le_Developer'], axis=1)\n",
    "df_f_train.shape, df_f_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_f_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n",
    "    if prefix: format_str = str(prefix) + format_str\n",
    "    if suffix: format_str = format_str + str(suffix)\n",
    "    start = time()\n",
    "    yield\n",
    "    d = time() - start\n",
    "    out_str = format_str.format(d)\n",
    "    if logger:\n",
    "        logger.info(out_str)\n",
    "    else:\n",
    "        print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mykaggle.feature.te import TE\n",
    "from mykaggle.feature.te_other_aggs import TEOtherAggs\n",
    "from mykaggle.feature.te_year_platform import TEYearPlatform\n",
    "from mykaggle.feature.year_rank4 import YearRank4\n",
    "\n",
    "\n",
    "te_train = TE(train=True)\n",
    "te_test = TE(train=False)\n",
    "te_oa_train = TEOtherAggs(train=True)\n",
    "te_oa_test = TEOtherAggs(train=False)\n",
    "te_yp_train = TEYearPlatform(train=True)\n",
    "te_yp_test = TEYearPlatform(train=False)\n",
    "year_rank_train = YearRank4(train=True)\n",
    "year_rank_test = YearRank4(train=False)\n",
    "\n",
    "\n",
    "def get_oof_feature(\n",
    "    df_train: pd.DataFrame, df_valid: pd.DataFrame, y_train: np.ndarray,\n",
    "    df_train_original: pd.DataFrame, df_valid_original: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_train.loc[:, 'target'] = y_train.copy()\n",
    "    _train_others = copy.deepcopy(train_others)\n",
    "    _test_others = copy.deepcopy(test_others)\n",
    "    _train_others['main'] = df_train_original.copy().reset_index().drop('index', axis=1)\n",
    "    _train_others['another'] = df_valid_original.copy().reset_index().drop('index', axis=1)\n",
    "    _test_others['main'] = df_valid_original.copy().reset_index().drop('index', axis=1)\n",
    "    _test_others['another'] = df_train_original.copy().reset_index().drop('index', axis=1)\n",
    "    df_train = df_train.reset_index().drop('index', axis=1)\n",
    "    df_valid = df_valid.reset_index().drop('index', axis=1)\n",
    "    df_train = te_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = te_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    df_train = te_oa_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = te_oa_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    df_train = te_yp_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = te_yp_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    df_train = year_rank_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = year_rank_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    return df_train.drop(['id', 'target'], axis=1), df_valid.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    settings: Dict[str, Any],\n",
    "    logger: MLLogger,\n",
    "    df: pd.DataFrame,\n",
    "    df_original: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    ckptdir: Path\n",
    ") -> Tuple:\n",
    "    logger.log_params(settings['training'])\n",
    "    logger.log_params(settings['lgbm_params'])\n",
    "    X = df.copy()\n",
    "    lgbm_params = settings['lgbm_params']\n",
    "    models = []\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "    importances = pd.DataFrame()\n",
    "    split_file = f'{settings[\"training\"][\"validation\"]}_{settings[\"training\"][\"num_folds\"]}fold.pkl'\n",
    "    splits = pickle.load(open(datadir / split_file, 'rb'))\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits): \n",
    "        x_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "        x_valid, y_valid = X.iloc[valid_idx], y[valid_idx]\n",
    "        x_train, x_valid = get_oof_feature(\n",
    "            x_train, x_valid, y_train,\n",
    "            df_original.iloc[train_idx], df_original.iloc[valid_idx]\n",
    "        )\n",
    "        train_data = lgb.Dataset(x_train.values, label=y_train)\n",
    "        valid_data = lgb.Dataset(x_valid.values, label=y_valid)\n",
    "\n",
    "        with timer(prefix='train fold={} '.format(i + 1)):\n",
    "            clf = lgb.train(\n",
    "                lgbm_params,\n",
    "                train_data, \n",
    "                num_boost_round=settings['training']['num_rounds'],\n",
    "                valid_names=['train', 'valid'],\n",
    "                valid_sets=[train_data, valid_data],  \n",
    "                early_stopping_rounds=settings['training']['early_stopping_rounds'],\n",
    "                feval=rmse,\n",
    "                verbose_eval=settings['training']['verbose_eval']\n",
    "            )\n",
    "        pred_i = clf.predict(x_valid.values)\n",
    "        oof_pred[valid_idx] = pred_i\n",
    "        models.append(clf)\n",
    "        importances = compute_importances(importances, x_train.columns, models[i], fold=i)\n",
    "        fold_score = mean_squared_error(y_valid, pred_i) ** 0.5\n",
    "        logger.log_metric(f'rmsle_fold_{i}', fold_score)\n",
    "        print(f'Fold {i} RMSLE: {fold_score:.4f}')\n",
    "        clf.save_model(str(ckptdir / f'model.txt'))\n",
    "        logger.log_artifact(str(ckptdir / f'model.txt'), artifact_path=f'{i}')\n",
    "\n",
    "    score = mean_squared_error(y, oof_pred) ** 0.5\n",
    "    logger.log_metric(f'RMSLE', score)\n",
    "    print('FINISHED; whole score: {:.4f}'.format(score))\n",
    "    save_importances(importances, ckptdir)\n",
    "    return oof_pred, models, score, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_oof(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray\n",
    ") -> None:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot([-100, 5000], [-100, 5000], color='black')\n",
    "    plt.scatter(y_true, y_pred, alpha=0.2)\n",
    "    plt.xlim(-100, 4000)\n",
    "    plt.ylim(-100, 4000)\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Pred')\n",
    "\n",
    "\n",
    "def predict(\n",
    "    models: List[lgb.Booster],\n",
    "    df_test: pd.DataFrame,\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test_original: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    fold 分のモデルと test の特徴 dataframe を受け取って、予測したものを返します。\n",
    "    :param models: kfold 分のモデル\n",
    "    :param test_df: test dataset の特徴\n",
    "    :return: 予測\n",
    "    '''\n",
    "    _, test = get_oof_feature(\n",
    "        df_train.copy(), df_test.copy(), df_train['target'].values, df_train.copy(), df_test_original.copy()\n",
    "    )\n",
    "    preds = np.array([model.predict(test) for model in models])\n",
    "    preds = np.expm1(preds)\n",
    "    preds = np.mean(preds, axis=0)\n",
    "    return preds\n",
    "\n",
    "    \n",
    "def submit(preds: np.ndarray, ckptdir: Path) -> pd.DataFrame:\n",
    "    '''\n",
    "    test data の prediction を受け取って submission ファイルを作成します。\n",
    "    :param preds: test data の予測\n",
    "    :param ckptdir: 保存場所\n",
    "    :return: submission df\n",
    "    '''\n",
    "    sub_df = pd.DataFrame({ 'Global_Sales': preds })\n",
    "    sub_df.to_csv(ckptdir / f'{settings[\"name\"]}.csv', index=False)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = MLLogger('cfiken', ckptdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with logger.start(experiment_name=settings['competition'], run_name=settings['name']):\n",
    "    oof, models, cv, importances = train(\n",
    "        settings,\n",
    "        logger,\n",
    "        df_f_train,\n",
    "        df_train,\n",
    "        df_train.loc[:, 'target'].values,\n",
    "        ckptdir\n",
    "    )\n",
    "    preds = predict(models, df_f_test, df_train, df_test)\n",
    "    sub_df = submit(preds, ckptdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_oof(np.expm1(df_train.loc[:, 'target'].values), np.expm1(oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
