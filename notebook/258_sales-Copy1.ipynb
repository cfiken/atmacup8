{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from pandas_profiling import ProfileReport # profile report を作る用\n",
    "from matplotlib_venn import venn2 # venn図を作成する用\n",
    "from tqdm import tqdm\n",
    "from contextlib import contextmanager\n",
    "from time import time\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_log_error, mean_squared_error\n",
    "import lightgbm as lgb\n",
    "\n",
    "from mykaggle.metric.mse import rmse\n",
    "from mykaggle.util.ml_logger import MLLogger\n",
    "from mykaggle.lib.lgbm_util import compute_importances, save_importances\n",
    "from mykaggle.util.routine import fix_seed\n",
    "\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"258_2_sales\",\n",
      "    \"competition\": \"atmacup8\",\n",
      "    \"description\": \"\",\n",
      "    \"seed\": 1019,\n",
      "    \"training\": {\n",
      "        \"pca_dim\": 4,\n",
      "        \"validation\": \"group\",\n",
      "        \"num_folds\": 5,\n",
      "        \"num_rounds\": 10000,\n",
      "        \"early_stopping_rounds\": 1000,\n",
      "        \"verbose_eval\": 20,\n",
      "        \"sample_weight_division\": 0.01,\n",
      "        \"sample_weight_threshold\": 1000\n",
      "    },\n",
      "    \"feature\": {\n",
      "        \"name_bow_pca_dim\": 4,\n",
      "        \"name_bow_word_th1\": 5,\n",
      "        \"name_bow_word_th2\": 3,\n",
      "        \"name_bow_th1_upper\": 130,\n",
      "        \"name_bow_th2_upper\": 100\n",
      "    },\n",
      "    \"lgbm_params\": {\n",
      "        \"objective\": \"regression\",\n",
      "        \"learning_rate\": 0.01,\n",
      "        \"max_depth\": -1,\n",
      "        \"num_leaves\": 31,\n",
      "        \"colsample_bytree\": 0.7,\n",
      "        \"metric\": \"None\"\n",
      "    }\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/workspace/atmacup8/.venv/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "seed = 1019\n",
    "\n",
    "settings = yaml.safe_load(f'''\n",
    "name: '258_2_sales'\n",
    "competition: atmacup8\n",
    "description: ''\n",
    "seed: {seed}\n",
    "training:\n",
    "    pca_dim: 4\n",
    "    validation: 'group'\n",
    "    num_folds: 5\n",
    "    num_rounds: 10000\n",
    "    early_stopping_rounds: 1000\n",
    "    verbose_eval: 20\n",
    "    sample_weight_division: 0.01  # 10/2/1\n",
    "    sample_weight_threshold: 1000\n",
    "feature:\n",
    "    name_bow_pca_dim: 4\n",
    "    name_bow_word_th1: 5\n",
    "    name_bow_word_th2: 3\n",
    "    name_bow_th1_upper: 130\n",
    "    name_bow_th2_upper: 100\n",
    "lgbm_params:\n",
    "    # seed: {seed}\n",
    "    objective: regression\n",
    "    learning_rate: 0.01\n",
    "    max_depth: -1\n",
    "    num_leaves: 31\n",
    "    colsample_bytree: .7\n",
    "    metric: \"None\"\n",
    "''')\n",
    "print(json.dumps(settings, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_seed(settings['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = Path('../data/')\n",
    "ckptdir = Path('../ckpt/') / settings['name']\n",
    "if not ckptdir.exists():\n",
    "    ckptdir.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8359, 21), (8360, 16))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(datadir / 'country_prob_train.csv')\n",
    "df_test = pd.read_csv(datadir / 'country_prob_test_unknown.csv')\n",
    "df_submission = pd.read_csv(datadir / 'atmaCup8_sample-submission.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target や weight の用意\n",
    "target_column = 'Global_Sales'\n",
    "train_target_columns = ['Global_Sales', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']\n",
    "\n",
    "for c in train_target_columns:\n",
    "    df_train.loc[:, 'mod_' + c] = df_train.loc[:, c].apply(lambda x: np.log1p(x))\n",
    "df_train['target'] = df_train.loc[:, f'mod_{target_column}']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base_train = df_train.loc[:, ['id']]\n",
    "df_base_test = df_test.loc[:, ['id']]\n",
    "\n",
    "train_others = {\n",
    "    'main': df_train.copy(),\n",
    "    'another': df_test.copy()\n",
    "}\n",
    "test_others = {\n",
    "    'main': df_test.copy(),\n",
    "    'another': df_train.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/workspace/atmacup8/.venv/lib/python3.8/site-packages/scipy/sparse/sparsetools.py:21: DeprecationWarning: `scipy.sparse.sparsetools` is deprecated!\n",
      "scipy.sparse.sparsetools is a private module for scipy.sparse, and should not be used.\n",
      "  _deprecated()\n"
     ]
    }
   ],
   "source": [
    "from mykaggle.feature.le import LE\n",
    "# from mykaggle.feature.score import Score\n",
    "from mykaggle.feature.score_nan import ScoreNaN\n",
    "from mykaggle.feature.score_tbd import ScoreTBD\n",
    "from mykaggle.feature.multi_platform import MultiPlatform\n",
    "from mykaggle.feature.other_platforms import OtherPlatforms\n",
    "from mykaggle.feature.ce import CE\n",
    "# from mykaggle.feature.original import Original\n",
    "from mykaggle.feature.year_nan import YearNaN\n",
    "from mykaggle.feature.platform_to_pub_genre_once import PlatformToPubGenreOnce\n",
    "from mykaggle.feature.genre_to_all_once import GenreToAllOnce\n",
    "from mykaggle.feature.dev_to_category import DevToCategory\n",
    "from mykaggle.feature.pub_to_category import PubToCategory\n",
    "from mykaggle.feature.pub_count_500 import PubCount500\n",
    "from mykaggle.feature.dev_to_category_pivot_pca_all import DevToCategoryPivotPCAAll\n",
    "from mykaggle.feature.pub_to_category_pivot_pca_all import PubToCategoryPivotPCAAll\n",
    "from mykaggle.feature.name_bow2 import NameBOW2\n",
    "from mykaggle.feature.has_sales import HasSales\n",
    "from mykaggle.feature.name_series_count import NameSeriesCount\n",
    "from mykaggle.feature.platform_to_scores import PlatformToScores\n",
    "from mykaggle.feature.platform_to_scores_diff import PlatformToScoresDiff\n",
    "from mykaggle.feature.year_to_scores import YearToScores\n",
    "from mykaggle.feature.year_to_scores_diff import YearToScoresDiff\n",
    "from mykaggle.feature.platform_year_to_scores import PlatformYearToScores\n",
    "from mykaggle.feature.region_sales import RegionSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/workspace/atmacup8/.venv/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "fhp = settings['feature']\n",
    "\n",
    "yearnan_train = YearNaN(train=True)\n",
    "yearnan_test = YearNaN(train=False)\n",
    "le_train = LE(train=True)\n",
    "le_test = LE(train=False)\n",
    "# score_train = Score(train=True)\n",
    "# score_test = Score(train=False)\n",
    "score_train = ScoreNaN(train=True)\n",
    "score_test = ScoreNaN(train=False)\n",
    "score_tbd_train = ScoreTBD(train=True)\n",
    "score_tbd_test = ScoreTBD(train=False)\n",
    "mp_train = MultiPlatform(train=True)\n",
    "mp_test = MultiPlatform(train=False)\n",
    "op_train = OtherPlatforms(train=True)\n",
    "op_test = OtherPlatforms(train=False)\n",
    "ce_train = CE(train=True)\n",
    "ce_test = CE(train=False)\n",
    "dtc_train = DevToCategory(train=True)\n",
    "dtc_test = DevToCategory(train=False)\n",
    "ptc_train = PubToCategory(train=True)\n",
    "ptc_test = PubToCategory(train=False)\n",
    "dtcpa_train = DevToCategoryPivotPCAAll(train=True, n_components=2)\n",
    "dtcpa_test = DevToCategoryPivotPCAAll(train=False, n_components=2)\n",
    "ptcpa_train = PubToCategoryPivotPCAAll(train=True, n_components=settings['training']['pca_dim'])\n",
    "ptcpa_test = PubToCategoryPivotPCAAll(train=False, n_components=settings['training']['pca_dim'])\n",
    "pftao_train = PlatformToPubGenreOnce(train=True, n_components=4)\n",
    "pftao_test = PlatformToPubGenreOnce(train=False, n_components=4)\n",
    "gtao_train = GenreToAllOnce(train=True, n_components=4)\n",
    "gtao_test = GenreToAllOnce(train=False, n_components=4)\n",
    "p500_train = PubCount500(train=True)\n",
    "p500_test = PubCount500(train=False)\n",
    "nbow_train = NameBOW2(\n",
    "    True, fhp['name_bow_pca_dim'], fhp['name_bow_word_th1'], fhp['name_bow_word_th2'],\n",
    "    fhp['name_bow_th1_upper'], fhp['name_bow_th2_upper'],\n",
    ")\n",
    "nbow_test = NameBOW2(\n",
    "    False, fhp['name_bow_pca_dim'], fhp['name_bow_word_th1'], fhp['name_bow_word_th2'],\n",
    "    fhp['name_bow_th1_upper'], fhp['name_bow_th2_upper'],\n",
    ")\n",
    "has_sales_train = HasSales(train=True)\n",
    "has_sales_test = HasSales(train=False)\n",
    "nsc_train = NameSeriesCount(train=True)\n",
    "nsc_test = NameSeriesCount(train=False)\n",
    "ptscore_train = PlatformToScores(train=True)\n",
    "ptscore_test = PlatformToScores(train=False)\n",
    "ptscored_train = PlatformToScoresDiff(train=True)\n",
    "ptscored_test = PlatformToScoresDiff(train=False)\n",
    "yscore_train = YearToScores(train=True)\n",
    "yscore_test = YearToScores(train=False)\n",
    "yscored_train = YearToScoresDiff(train=True)\n",
    "yscored_test = YearToScoresDiff(train=False)\n",
    "pyscore_train = PlatformYearToScores(train=True)\n",
    "pyscore_test = PlatformYearToScores(train=False)\n",
    "rsales_train = RegionSales(train=True)\n",
    "rsales_test = RegionSales(train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../ckpt/256_na_sales/train_256_na_sales.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2a468e01f349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0mdf_f_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyscore_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_f_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_others\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mdf_f_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyscore_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_f_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_others\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdf_f_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrsales_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_f_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_others\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mdf_f_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrsales_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_f_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_others\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/mykaggle/feature/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, base, others, use_cache, save_cache, merge, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mfeature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mothers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msave_cache\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/mykaggle/feature/region_sales.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, base, others, *args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mdf_jp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../ckpt/248_jp_sales/train_248_jp_sales.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mdf_na\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../ckpt/256_na_sales/train_256_na_sales.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mdf_eu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../ckpt/252_eu_sales/train_252_eu_sales.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mdf_other\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../ckpt/253_others_sales/train_253_others_sales.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/workspace/atmacup8/.venv/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../ckpt/256_na_sales/train_256_na_sales.csv'"
     ]
    }
   ],
   "source": [
    "df_f_train = df_base_train.copy()\n",
    "df_f_test = df_base_test.copy()\n",
    "\n",
    "df_f_train = yearnan_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = yearnan_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = le_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = le_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = score_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = score_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = score_tbd_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = score_tbd_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = mp_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = mp_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = op_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = op_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ce_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ce_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = dtc_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = dtc_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ptc_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ptc_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = dtcpa_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = dtcpa_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ptcpa_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ptcpa_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = pftao_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = pftao_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = gtao_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = gtao_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = p500_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = p500_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = nbow_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = nbow_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = has_sales_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = has_sales_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = nsc_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = nsc_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ptscore_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ptscore_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = ptscored_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = ptscored_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = yscore_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = yscore_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = yscored_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = yscored_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = pyscore_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = pyscore_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "df_f_train = rsales_train(df_f_train, others=train_others, use_cache=False, save_cache=True)\n",
    "df_f_test = rsales_test(df_f_test, others=test_others, use_cache=False, save_cache=True)\n",
    "\n",
    "df_f_train.shape, df_f_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop some columns\n",
    "df_f_train = df_f_train.drop(['le_Developer'], axis=1)\n",
    "df_f_test = df_f_test.drop(['le_Developer'], axis=1)\n",
    "df_f_train.shape, df_f_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "df_f_train.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(logger=None, format_str='{:.3f}[s]', prefix=None, suffix=None):\n",
    "    if prefix: format_str = str(prefix) + format_str\n",
    "    if suffix: format_str = format_str + str(suffix)\n",
    "    start = time()\n",
    "    yield\n",
    "    d = time() - start\n",
    "    out_str = format_str.format(d)\n",
    "    if logger:\n",
    "        logger.info(out_str)\n",
    "    else:\n",
    "        print(out_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mykaggle.feature.te import TE\n",
    "from mykaggle.feature.te_other_aggs import TEOtherAggs\n",
    "from mykaggle.feature.te_year_platform import TEYearPlatform\n",
    "from mykaggle.feature.te_year_genre import TEYearGenre\n",
    "from mykaggle.feature.year_rank6 import YearRank6\n",
    "from mykaggle.feature.platform_time_diff import PlatformTimeDiff\n",
    "from mykaggle.feature.platform_to_region_sales import PlatformToRegionSales\n",
    "\n",
    "\n",
    "te_train = TE(train=True)\n",
    "te_test = TE(train=False)\n",
    "te_oa_train = TEOtherAggs(train=True)\n",
    "te_oa_test = TEOtherAggs(train=False)\n",
    "# te_yp_train = TEYearPlatform(train=True)\n",
    "# te_yp_test = TEYearPlatform(train=False)\n",
    "# te_yg_train = TEYearGenre(train=True)\n",
    "# te_yg_test = TEYearGenre(train=False)\n",
    "year_rank_train = YearRank6(train=True)\n",
    "year_rank_test = YearRank6(train=False)\n",
    "ptdiff_train = PlatformTimeDiff(train=True)\n",
    "ptdiff_test = PlatformTimeDiff(train=False)\n",
    "ptrs_train = PlatformToRegionSales(train=True)\n",
    "ptrs_test = PlatformToRegionSales(train=False)\n",
    "\n",
    "\n",
    "def get_oof_feature(\n",
    "    df_train: pd.DataFrame, df_valid: pd.DataFrame, y_train: np.ndarray,\n",
    "    df_train_original: pd.DataFrame, df_valid_original: pd.DataFrame\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    df_train.loc[:, 'target'] = y_train.copy()\n",
    "    _train_others = copy.deepcopy(train_others)\n",
    "    _test_others = copy.deepcopy(test_others)\n",
    "    _train_others['main'] = df_train_original.copy().reset_index().drop('index', axis=1)\n",
    "    _train_others['another'] = df_valid_original.copy().reset_index().drop('index', axis=1)\n",
    "    _test_others['main'] = df_valid_original.copy().reset_index().drop('index', axis=1)\n",
    "    _test_others['another'] = df_train_original.copy().reset_index().drop('index', axis=1)\n",
    "    df_train = df_train.reset_index().drop('index', axis=1)\n",
    "    df_valid = df_valid.reset_index().drop('index', axis=1)\n",
    "    df_train = te_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = te_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    df_train = te_oa_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = te_oa_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "#     df_train = te_yp_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "#     df_valid = te_yp_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "#     df_train = te_yg_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "#     df_valid = te_yg_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    df_train = year_rank_train(df_train, others=_train_others, use_cache=False, save_cache=False)\n",
    "    df_valid = year_rank_test(df_valid, others=_test_others, use_cache=False, save_cache=False)\n",
    "    df_train = ptdiff_train(df_train, others=_train_others, use_cache=False, save_cache=True)\n",
    "    df_valid = ptdiff_test(df_valid, others=_test_others, use_cache=False, save_cache=True)\n",
    "    df_train = ptrs_train(df_train, others=_train_others, use_cache=False, save_cache=True)\n",
    "    df_valid = ptrs_test(df_valid, others=_test_others, use_cache=False, save_cache=True)\n",
    "    return df_train.drop(['id', 'target'], axis=1), df_valid.drop('id', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    settings: Dict[str, Any],\n",
    "    logger: MLLogger,\n",
    "    df: pd.DataFrame,\n",
    "    df_original: pd.DataFrame,\n",
    "    y: np.ndarray,\n",
    "    ckptdir: Path\n",
    ") -> Tuple:\n",
    "    logger.log_params(settings['training'])\n",
    "    logger.log_params(settings['feature'])\n",
    "    logger.log_params(settings['lgbm_params'])\n",
    "    X = df.copy()\n",
    "    lgbm_params = settings['lgbm_params']\n",
    "    models = []\n",
    "    oof_pred = np.zeros_like(y, dtype=np.float)\n",
    "    importances = pd.DataFrame()\n",
    "    split_file = f'{settings[\"training\"][\"validation\"]}_{settings[\"training\"][\"num_folds\"]}fold.pkl'\n",
    "    splits = pickle.load(open(datadir / split_file, 'rb'))\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(splits): \n",
    "        x_train, y_train = X.iloc[train_idx], y[train_idx]\n",
    "        x_valid, y_valid = X.iloc[valid_idx], y[valid_idx]\n",
    "        x_train, x_valid = get_oof_feature(\n",
    "            x_train, x_valid, y_train,\n",
    "            df_original.iloc[train_idx], df_original.iloc[valid_idx]\n",
    "        )\n",
    "        train_data = lgb.Dataset(x_train.values, label=y_train)\n",
    "        valid_data = lgb.Dataset(x_valid.values, label=y_valid)\n",
    "\n",
    "        with timer(prefix='train fold={} '.format(i + 1)):\n",
    "            clf = lgb.train(\n",
    "                lgbm_params,\n",
    "                train_data, \n",
    "                num_boost_round=settings['training']['num_rounds'],\n",
    "                valid_names=['train', 'valid'],\n",
    "                valid_sets=[train_data, valid_data],  \n",
    "                early_stopping_rounds=settings['training']['early_stopping_rounds'],\n",
    "                feval=rmse,\n",
    "                verbose_eval=settings['training']['verbose_eval']\n",
    "            )\n",
    "        pred_i = clf.predict(x_valid.values)\n",
    "        oof_pred[valid_idx] = pred_i\n",
    "        models.append(clf)\n",
    "        importances = compute_importances(importances, x_train.columns, models[i], fold=i)\n",
    "        fold_score = mean_squared_error(y_valid, pred_i) ** 0.5\n",
    "        logger.log_metric(f'rmsle_fold_{i}', fold_score)\n",
    "        print(f'Fold {i} RMSLE: {fold_score:.4f}')\n",
    "        clf.save_model(str(ckptdir / f'model.txt'))\n",
    "        logger.log_artifact(str(ckptdir / f'model.txt'), artifact_path=f'{i}')\n",
    "\n",
    "    score = mean_squared_error(y, oof_pred) ** 0.5\n",
    "    logger.log_metric(f'RMSLE', score)\n",
    "    print('FINISHED; whole score: {:.4f}'.format(score))\n",
    "    save_importances(importances, ckptdir)\n",
    "    return oof_pred, models, score, importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_oof(\n",
    "    y_true: np.ndarray, y_pred: np.ndarray\n",
    ") -> None:\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot([-100, 5000], [-100, 5000], color='black')\n",
    "    plt.scatter(y_true, y_pred, alpha=0.2)\n",
    "    plt.xlim(-100, 4000)\n",
    "    plt.ylim(-100, 4000)\n",
    "    plt.xlabel('True')\n",
    "    plt.ylabel('Pred')\n",
    "\n",
    "\n",
    "def predict(\n",
    "    models: List[lgb.Booster],\n",
    "    df_test: pd.DataFrame,\n",
    "    df_train: pd.DataFrame,\n",
    "    df_test_original: pd.DataFrame\n",
    ") -> np.ndarray:\n",
    "    '''\n",
    "    fold 分のモデルと test の特徴 dataframe を受け取って、予測したものを返します。\n",
    "    :param models: kfold 分のモデル\n",
    "    :param test_df: test dataset の特徴\n",
    "    :return: 予測\n",
    "    '''\n",
    "    _, test = get_oof_feature(\n",
    "        df_train.copy(), df_test.copy(), df_train['target'].values, df_train.copy(), df_test_original.copy()\n",
    "    )\n",
    "    preds = np.array([model.predict(test) for model in models])\n",
    "    preds = np.expm1(preds)\n",
    "    preds = np.mean(preds, axis=0)\n",
    "    return preds\n",
    "\n",
    "    \n",
    "def submit(preds: np.ndarray, ckptdir: Path) -> pd.DataFrame:\n",
    "    '''\n",
    "    test data の prediction を受け取って submission ファイルを作成します。\n",
    "    :param preds: test data の予測\n",
    "    :param ckptdir: 保存場所\n",
    "    :return: submission df\n",
    "    '''\n",
    "    sub_df = pd.DataFrame({ target_column: preds })\n",
    "    sub_df.to_csv(ckptdir / f'{settings[\"name\"]}.csv', index=False)\n",
    "    return sub_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = MLLogger('cfiken', ckptdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with logger.start(experiment_name=settings['competition'], run_name=settings['name']):\n",
    "    oof, models, cv, importances = train(\n",
    "        settings,\n",
    "        logger,\n",
    "        df_f_train,\n",
    "        df_train,\n",
    "        df_train.loc[:, 'target'].values,\n",
    "        ckptdir\n",
    "    )\n",
    "    preds = predict(models, df_f_test, df_train, df_test)\n",
    "    sub_df = submit(preds, ckptdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_oof(np.expm1(df_train.loc[:, 'target'].values), np.expm1(oof))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
